{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports \n",
    "import sys\n",
    "from importlib import reload\n",
    "#import findspark\n",
    "import customHelpers as helper\n",
    "#findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType,BooleanType,DateType\n",
    "\n",
    "reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the session \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Amazon Product Review Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset \n",
    "\n",
    "| Column | Description | \n",
    "| :--- | :--- |\n",
    "| marketplace | 2 letter country code of the marketplace where the review was written. |\n",
    "| customer_id | Random identifier that can be used to aggregate reviews written by a single author. |\n",
    "| review_id | The unique ID of the review. |\n",
    "| product_id | The unique Product ID the review pertains to. In the multilingual dataset the reviews for the same product in different countries can be grouped by the same product_id. | \n",
    "| product_parent | Random identifier that can be used to aggregate reviews for the same product. |\n",
    "| product_title | Title of the product. | \n",
    "| product_category | Broad product category that can be used to group reviews (also used to group the dataset into  coherent parts). | \n",
    "| star_rating | the 1-5 star rating of the review. | \n",
    "| helpful_votes | Number of helpful votes. | \n",
    "| total_votes | Number of total votes the review received. | \n",
    "| vine | Review was written as part of the Vine program. |\n",
    "| verified_purchase | The review is on a verified purchase. |\n",
    "| review_headline | The title of the review. |\n",
    "| review_body | The review text. |\n",
    "| review_date | The date the review was written | \n",
    "\n",
    "\n",
    "DATA FORMAT\n",
    "Tab ('\\t') separated text file, without quote or escape characters.\n",
    "First line in each file is header; 1 line corresponds to 1 record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data set \n",
    "#review_data = '../data/sample_us.tsv'\n",
    "# actual data load - PERFORMANCE WARNING ON LOCAL MACHINE\n",
    "review_data = '../data/amazon_reviews_us_Music_v1_00.tsv'\n",
    "\n",
    "aws_product_review_schema = StructType([\n",
    "    StructField(\"marketplace\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\",StringType(),True),\n",
    "    StructField(\"product_parent\",StringType(),False),\n",
    "    StructField(\"product_title\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"star_rating\", IntegerType(), False),\n",
    "    StructField(\"helpful_votes\",IntegerType(),False),\n",
    "    StructField(\"total_votes\", IntegerType(), False),\n",
    "    StructField(\"vine\",StringType(),False),\n",
    "    StructField(\"verified_purchase\", StringType(), False),\n",
    "    StructField(\"review_headline\", StringType(), False),\n",
    "    StructField(\"review_body\", StringType(), False),\n",
    "    StructField(\"review_date\",DateType(),False)])\n",
    "\n",
    "aws_product_review_schema_limited = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\",StringType(),True),\n",
    "    StructField(\"product_title\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"star_rating\", IntegerType(), False),\n",
    "    StructField(\"helpful_votes\",IntegerType(),False),\n",
    "    StructField(\"total_votes\", IntegerType(), False),\n",
    "    StructField(\"review_headline\", StringType(), False),\n",
    "    StructField(\"review_body\", StringType(), False),\n",
    "    StructField(\"review_date\",DateType(),False)])\n",
    "\n",
    "%time awsProductReview_raw_data = spark.read.csv(review_data,header=True,sep=\"\\t\",schema=aws_product_review_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when testing in local machine only \n",
    "print(awsProductReview_raw_data.count())\n",
    "# limit to 1 mil\n",
    "awsProductReview_raw_data = awsProductReview_raw_data.limit(100000)\n",
    "print(awsProductReview_raw_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProductReview = awsProductReview_raw_data.drop('vine').drop('verified_purchase') \\\n",
    "                    .drop('product_parent').drop('marketplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with no review text \n",
    "print(\"number of rows before filter: {0}\".format(dfProductReview.count()))\n",
    "dfFilteredReviews = dfProductReview.na.drop(subset=[\"review_body\"])\n",
    "print(\"number of rows after filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Overall statistics\n",
    "\n",
    "### Produce overall summary statistics of the data set, in particular,\n",
    "\n",
    "* the total number of reviews\n",
    "* the number of unique users\n",
    "* the number of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, when, isnull\n",
    "\n",
    "dfOverallStats = dfFilteredReviews.agg(countDistinct(\"customer_id\").alias(\"unique_customers\"), \\\n",
    "                    countDistinct(\"product_id\").alias(\"unique_products\"), \\\n",
    "                    count(col=\"review_id\").alias(\"total_reviews\")) \\\n",
    "\n",
    "dfOverallStats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IN ALL THE COLUMNS IF THERE IS ANY NULL VALUE.\n",
    "dfFilteredReviews.select([count(when(isnull(column), column)).alias(column) \\\n",
    "                        for column in dfProductReview.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For user-review distribution, you are asked to find out:\n",
    "\n",
    "* the largest number of reviews published by a single user\n",
    "* the top 10 users ranked by the number of reviews they publish\n",
    "* the median number of reviews published by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "dfUserReviewCounts = helper.distributionStats(dfRecords=dfFilteredReviews.select(\"customer_id\", \"review_id\"), \\\n",
    "                                              partitionBy=\"customer_id\",countBy=\"review_id\", \\\n",
    "                                              returnCountName=\"total_reviews\")\n",
    "print(\"Top Reviewer:\")\n",
    "dfUserReviewCounts.show(1)\n",
    "print(\"Top 10 Reviewers:\")\n",
    "dfUserReviewCounts.show(10)\n",
    "\n",
    "user_review_median=dfUserReviewCounts.approxQuantile(\"total_reviews\", [0.50], 0)[0]\n",
    "print(\"median number of {0} reviews published by user\".format(user_review_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For product-review distribution, you are asked to find out:\n",
    "    \n",
    "* the largest number of reviews written for a single product\n",
    "* the top 10 products ranked by the number of reviews they have\n",
    "* the median number of reviews a product has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProductReviewCounts = helper.distributionStats(dfRecords=dfProductReview.select(\"product_id\", \"review_id\"), \\\n",
    "                                              partitionBy=\"product_id\",countBy=\"review_id\", \\\n",
    "                                              returnCountName=\"total_reviews\")\n",
    "print(\"Top Product By Review:\")\n",
    "dfProductReviewCounts.show(1)\n",
    "print(\"Top 10 Products by Reviews:\")\n",
    "dfProductReviewCounts.show(10)\n",
    "\n",
    "product_review_median=int(dfProductReviewCounts.approxQuantile(\"total_reviews\", [0.50], 0)[0])\n",
    "print(\"median number of {0} reviews per product\".format(product_review_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Filtering Unwanted Data\n",
    "\n",
    "filter reviews based on length, reviewer and product feature. In particular, the following reviews should be removed:\n",
    "\n",
    "* reviews with less than two sentences in the review body.\n",
    "* reviews published by users with less than median number of reviews published\n",
    "* reviews from products with less than median number of reviews received\n",
    "\n",
    "NOTE: Sentence Segmentation Using: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews with less than 2 sentences in review_body\n",
    "# convert to RDD and carry out a filter to remove rows with less than 2 sentences \n",
    "\n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "dfFilteredReviews = dfFilteredReviews.filter(helper.FilterSentences('review_body'))\n",
    "\n",
    "dfFilteredReviews.show(1)\n",
    "dfFilteredReviews.cache()\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user review filter \n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\")\n",
    "dfFilteredReviews = dfFilteredReviews \\\n",
    "    .withColumn(\"review_count\", count(\"review_id\") \\\n",
    "    .over(window)) \\\n",
    "    .filter(col(\"review_count\") >= user_review_median) \\\n",
    "    .drop(\"review_count\")\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product review filter\n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "window = Window.partitionBy(\"product_id\")\n",
    "dfFilteredReviews = dfFilteredReviews \\\n",
    "    .withColumn(\"review_count\", count(\"review_id\") \\\n",
    "    .over(window)) \\\n",
    "    .filter(col(\"review_count\") >= product_review_median) \\\n",
    "    .drop(\"review_count\")\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Number of Rows before cleanup: {0}\".format(dfProductReview.count()))\n",
    "print(\"Number of rows after all filters applied: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFilteredReviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After filtering out the above, find out:\n",
    "\n",
    "* top 10 users ranked by median number of sentences in the reviews they have published\n",
    "* top 10 products ranked by median number of sentences in the reviews they have received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 users ranked by median number of sentences in the reviews they have published\n",
    "dfTop10UsersBySents = helper.getTopBySentMedian(dfRecords=dfFilteredReviews, \\\n",
    "                                                partitionBy=\"customer_id\", \\\n",
    "                                                textCol=\"review_body\",\\\n",
    "                                                medianColName=\"median_sents\",n=10)\n",
    "dfTop10UsersBySents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 products ranked by median number of sentences in the reviews they have received\n",
    "dfTop10ProductsBySents = helper.getTopBySentMedian(dfRecords=dfFilteredReviews, \\\n",
    "                                                   partitionBy=\"product_id\", \\\n",
    "                                                   textCol=\"review_body\", \\\n",
    "                                                   medianColName=\"median_sents\",n=10)\n",
    "\n",
    "dfTop10ProductsBySents.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned and filtered dataframe to file system \n",
    "\n",
    "#dfFilteredReviews.coalesce(1).write.format(\"parquet\") \\\n",
    "#    .option(\"header\", \"true\").saveAsTable('filteredReviews',mode=\"overwrite\")\n",
    "dfFilteredReviews.coalesce(1).write.csv(\"../output\",mode=\"overwrite\",header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 Similarity analysis with Sentence Embedding\n",
    "\n",
    "perform similarity analysis on the review sentences. The analysis involves segmenting review body into multiple sentences; encoding each sentence as vector so that the distance between pair of sentences can be computed.\n",
    "\n",
    "### Positive vs. Negative Reviews\n",
    "\n",
    "* pick a product from the top 10 products in stage 1\n",
    "* Create a positive and negative class of reviews using the rating \n",
    "    - Positive Class - rate >=4 \n",
    "    - Negative Class - rate <= 2\n",
    "    - for each review, extracting the review body part and segment it into multiple sentences.\n",
    "    - encode the sentences using google universal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = \"../output/part-*.csv\"\n",
    "dfBaseDataset = spark.read.csv(filtered_data,header=True,sep=\"\\t\",schema=aws_product_review_schema_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product from top 10 by review number \n",
    "base_product_id = \"B00MIA0KGY\"\n",
    "\n",
    "dfSelectedProduct = dfBaseDataset.where((col(\"product_id\") == base_product_id))\n",
    "dfPositiveClass = dfBaseDataset.where((col(\"product_id\") == base_product_id) & (col(\"star_rating\") >= 4))\n",
    "dfNegativeClass = dfBaseDataset.where((col(\"product_id\") == base_product_id) & (col(\"star_rating\") <= 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of reviews from {0}: {1}\".format(base_product_id, dfSelectedProduct.count()))\n",
    "print(\"number of positives reviews from {0}: {1}\".format(base_product_id,dfPositiveClass.count()))\n",
    "print(\"number of negatives reviews from {0}: {1}\".format(base_product_id,dfNegativeClass.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the sentences - similar to flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each review, extract the review body part and segment it into multiple sentences\n",
    "# extract the positive sentences\n",
    "dfPosSents = dfPositiveClass.select(\"review_id\",\"review_body\") \\\n",
    "    .withColumn(\"sentences\", helper.GenerateSentences(\"review_body\")) \\\n",
    "    .select(\"review_id\", F.explode_outer(\"sentences\").alias(\"sentence\"))\n",
    "\n",
    "print(\"number of sentences from positive reviews: {0}\".format(dfPosSents.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the negative sentences\n",
    "dfNegSents = dfNegativeClass.select(\"review_id\",\"review_body\") \\\n",
    "    .withColumn(\"sentences\", helper.GenerateSentences(\"review_body\")) \\\n",
    "    .select(\"review_id\", F.explode_outer(\"sentences\").alias(\"sentence\"))\n",
    "\n",
    "print(\"number of sentences from negative reviews: {0}\".format(dfNegSents.count()))\n",
    "\n",
    "dfNegSents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the sentences - google universal encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType,BooleanType,DateType,ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the data for embedding: first step selecting the sentence column from the \n",
    "# data frames sentences. not sure why it doesn't work if don't use '.limit()' even if\n",
    "# dataframe is short \n",
    "pos_rev_text = dfPosSents.limit(1000).select('sentence')\n",
    "#second: convert the dataframe to rdd pipeline\n",
    "pos_rev_clean_text_rdd = pos_rev_text.rdd.map(lambda row: str(row[0])) \\\n",
    "                    .filter(lambda data: data is not None).cache()\n",
    "\n",
    "neg_rev_text = dfNegSents.limit(1000).select('review_id','sentence')\n",
    "#second: convert the dataframe to rdd pipeline\n",
    "neg_rev_clean_text_rdd = neg_rev_text.rdd.map(lambda row: row) \\\n",
    "                    .filter(lambda data: data is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function from lab 9\n",
    "def review_embed(rev_text_partition):\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "    # mapPartition would supply element inside a partition using generator stype\n",
    "    # this does not fit tensorflow stype\n",
    "    rev_text_list = [str(text[0]) for text in rev_text_partition]\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(rev_text_list))\n",
    "    \n",
    "    rev_textids_list = [str(text[0]) for text in rev_text_partition]\n",
    "\n",
    "    return rev_textids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the embedding from sentences\n",
    "#positive_review_embedding = pos_rev_clean_text_rdd.mapPartitions(review_embed).cache()\n",
    "negative_review_embedding = neg_rev_clean_text_rdd.mapPartitions(review_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_review_embedding.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Class Similarity\n",
    "\n",
    "We want to find out if sentences in the same category are closely related with each other. The closeness is measured by average distance between points in the class. In our case, point refers to the sentence encoding and pair-wise distance is measured by Cosine distance. Cosine distance is computed as “1 − CosineSimilarity”. It has a value between 0 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distances between vectors and average the results.\n",
    "\n",
    "def CosineDistance(embedded_sentences):\n",
    "    \n",
    "    # transform the numpy arrays into pyspark Vectors\n",
    "    embedded_sentences_vect = embedded_sentences.map(Vectors.dense).cache()\n",
    "    \n",
    "    # collect the cartesian combination of the vectors\n",
    "    # eg: (vect1, vect1), (vect1, vect2),(vect1, vect3), etc... \n",
    "    # I am using cartesian here to avoid nested for loops in the Cosine Distance calculation.\n",
    "    cartesian_vector = embedded_sentences_vect.cartesian(review_embedding_dense).collect()\n",
    "    \n",
    "    distances = []\n",
    "    for v in cartesian_vector:\n",
    "        if v[0] != v[1]:\n",
    "            distance = 1 - (v[0].dot(v[1])/(v[0].norm(2)*v[1].norm(2)))\n",
    "            distances.append(distance)\n",
    "\n",
    "    return sum(distances)/len(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dist = CosineDistance(positive_review_embedding)\n",
    "print(\"Average cosine distance between positive reviews: {0}\".format(pos_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dist = CosineDistance(negative_review_embedding)\n",
    "print(\"Average cosine distance between negative reviews: {0}\".format(neg_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Center Sentences\n",
    "\n",
    "Find out the class center and its 10 closest neighbours for positive and negative class respectively. We define class center as the point that has the smallest average distance to other points in the class. Again in this case point refers to the sentence encoding and pair-wise distance are measured by Cosine distance.\n",
    "The result should show the text of the center sentence, the review id it belongs to and its 10 closest neighbouring sentences text and their respective review id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
