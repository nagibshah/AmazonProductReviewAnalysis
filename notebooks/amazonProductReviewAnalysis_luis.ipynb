{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'customHelpers' from '/Users/luisedumuller/Documents/Studies/MDS/CloudComputing/Assignment2/COMP5349_AmazonProductReviewAnalysis/notebooks/customHelpers.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# library imports \n",
    "import sys\n",
    "from importlib import reload\n",
    "#import findspark\n",
    "import customHelpers as helper\n",
    "#findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType,BooleanType,DateType\n",
    "\n",
    "reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the session \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Amazon Product Review Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset \n",
    "\n",
    "| Column | Description | \n",
    "| :--- | :--- |\n",
    "| marketplace | 2 letter country code of the marketplace where the review was written. |\n",
    "| customer_id | Random identifier that can be used to aggregate reviews written by a single author. |\n",
    "| review_id | The unique ID of the review. |\n",
    "| product_id | The unique Product ID the review pertains to. In the multilingual dataset the reviews for the same product in different countries can be grouped by the same product_id. | \n",
    "| product_parent | Random identifier that can be used to aggregate reviews for the same product. |\n",
    "| product_title | Title of the product. | \n",
    "| product_category | Broad product category that can be used to group reviews (also used to group the dataset into  coherent parts). | \n",
    "| star_rating | the 1-5 star rating of the review. | \n",
    "| helpful_votes | Number of helpful votes. | \n",
    "| total_votes | Number of total votes the review received. | \n",
    "| vine | Review was written as part of the Vine program. |\n",
    "| verified_purchase | The review is on a verified purchase. |\n",
    "| review_headline | The title of the review. |\n",
    "| review_body | The review text. |\n",
    "| review_date | The date the review was written | \n",
    "\n",
    "\n",
    "DATA FORMAT\n",
    "Tab ('\\t') separated text file, without quote or escape characters.\n",
    "First line in each file is header; 1 line corresponds to 1 record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 996 Âµs, total: 2.91 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "# load the data set \n",
    "#review_data = '../data/sample_us.tsv'\n",
    "# actual data load - PERFORMANCE WARNING ON LOCAL MACHINE\n",
    "review_data = '../data/amazon_reviews_us_Music_v1_00.tsv'\n",
    "\n",
    "aws_product_review_schema = StructType([\n",
    "    StructField(\"marketplace\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\",StringType(),True),\n",
    "    StructField(\"product_parent\",StringType(),False),\n",
    "    StructField(\"product_title\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"star_rating\", IntegerType(), False),\n",
    "    StructField(\"helpful_votes\",IntegerType(),False),\n",
    "    StructField(\"total_votes\", IntegerType(), False),\n",
    "    StructField(\"vine\",StringType(),False),\n",
    "    StructField(\"verified_purchase\", StringType(), False),\n",
    "    StructField(\"review_headline\", StringType(), False),\n",
    "    StructField(\"review_body\", StringType(), False),\n",
    "    StructField(\"review_date\",DateType(),False)])\n",
    "\n",
    "aws_product_review_schema_limited = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\",StringType(),True),\n",
    "    StructField(\"product_title\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"star_rating\", IntegerType(), False),\n",
    "    StructField(\"helpful_votes\",IntegerType(),False),\n",
    "    StructField(\"total_votes\", IntegerType(), False),\n",
    "    StructField(\"review_headline\", StringType(), False),\n",
    "    StructField(\"review_body\", StringType(), False),\n",
    "    StructField(\"review_date\",DateType(),False)])\n",
    "\n",
    "%time awsProductReview_raw_data = spark.read.csv(review_data,header=True,sep=\"\\t\",schema=aws_product_review_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4751577\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# when testing in local machine only \n",
    "print(awsProductReview_raw_data.count())\n",
    "# limit to 1 mil\n",
    "awsProductReview_raw_data = awsProductReview_raw_data.limit(100000)\n",
    "print(awsProductReview_raw_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProductReview = awsProductReview_raw_data.drop('vine').drop('verified_purchase') \\\n",
    "                    .drop('product_parent').drop('marketplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 100000\n",
      "number of rows after filter: 99977\n"
     ]
    }
   ],
   "source": [
    "# remove rows with no review text \n",
    "print(\"number of rows before filter: {0}\".format(dfProductReview.count()))\n",
    "dfFilteredReviews = dfProductReview.na.drop(subset=[\"review_body\"])\n",
    "print(\"number of rows after filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Overall statistics\n",
    "\n",
    "### Produce overall summary statistics of the data set, in particular,\n",
    "\n",
    "* the total number of reviews\n",
    "* the number of unique users\n",
    "* the number of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, when, isnull\n",
    "\n",
    "dfOverallStats = dfFilteredReviews.agg(countDistinct(\"customer_id\").alias(\"unique_customers\"), \\\n",
    "                    countDistinct(\"product_id\").alias(\"unique_products\"), \\\n",
    "                    count(col=\"review_id\").alias(\"total_reviews\")) \\\n",
    "\n",
    "#dfOverallStats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+---------------+-----------+-----------+\n",
      "|customer_id|review_id|product_id|product_title|product_category|star_rating|helpful_votes|total_votes|review_headline|review_body|review_date|\n",
      "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+---------------+-----------+-----------+\n",
      "|          0|        0|         0|            0|               0|          0|            0|          0|              0|          0|          0|\n",
      "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+---------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHECK IN ALL THE COLUMNS IF THERE IS ANY NULL VALUE.\n",
    "dfFilteredReviews.select([count(when(isnull(column), column)).alias(column) \\\n",
    "                        for column in dfProductReview.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For user-review distribution, you are asked to find out:\n",
    "\n",
    "* the largest number of reviews published by a single user\n",
    "* the top 10 users ranked by the number of reviews they publish\n",
    "* the median number of reviews published by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Reviewer:\n",
      "+-----------+-------------+\n",
      "|customer_id|total_reviews|\n",
      "+-----------+-------------+\n",
      "|   15536614|          373|\n",
      "+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Top 10 Reviewers:\n",
      "+-----------+-------------+\n",
      "|customer_id|total_reviews|\n",
      "+-----------+-------------+\n",
      "|   15536614|          373|\n",
      "|   38214553|          226|\n",
      "|    4276914|          222|\n",
      "|    8342883|          194|\n",
      "|    5291529|          190|\n",
      "|   13634768|          180|\n",
      "|    2112938|          126|\n",
      "|   38229524|           98|\n",
      "|   48046800|           97|\n",
      "|   22716027|           92|\n",
      "+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "median number of 1.0 reviews published by user\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "dfUserReviewCounts = helper.distributionStats(dfRecords=dfFilteredReviews.select(\"customer_id\", \"review_id\"), \\\n",
    "                                              partitionBy=\"customer_id\",countBy=\"review_id\", \\\n",
    "                                              returnCountName=\"total_reviews\")\n",
    "print(\"Top Reviewer:\")\n",
    "dfUserReviewCounts.show(1)\n",
    "print(\"Top 10 Reviewers:\")\n",
    "dfUserReviewCounts.show(10)\n",
    "\n",
    "user_review_median=dfUserReviewCounts.approxQuantile(\"total_reviews\", [0.50], 0)[0]\n",
    "print(\"median number of {0} reviews published by user\".format(user_review_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For product-review distribution, you are asked to find out:\n",
    "    \n",
    "* the largest number of reviews written for a single product\n",
    "* the top 10 products ranked by the number of reviews they have\n",
    "* the median number of reviews a product has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Product By Review:\n",
      "+----------+-------------+\n",
      "|product_id|total_reviews|\n",
      "+----------+-------------+\n",
      "|B00VXGTJMU|          256|\n",
      "+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Top 10 Products by Reviews:\n",
      "+----------+-------------+\n",
      "|product_id|total_reviews|\n",
      "+----------+-------------+\n",
      "|B00VXGTJMU|          256|\n",
      "|B00VMRJPCE|          255|\n",
      "|B00UCFVIDQ|          205|\n",
      "|B00VMQK37Q|          170|\n",
      "|B00ZQUP38S|          158|\n",
      "|B00WE2SMKC|          139|\n",
      "|B00WSOWR0M|          120|\n",
      "|B00MRHANNI|          116|\n",
      "|B00VTBBEL8|          115|\n",
      "|B00XJJAWES|          112|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "median number of 1 reviews per product\n"
     ]
    }
   ],
   "source": [
    "dfProductReviewCounts = helper.distributionStats(dfRecords=dfProductReview.select(\"product_id\", \"review_id\"), \\\n",
    "                                              partitionBy=\"product_id\",countBy=\"review_id\", \\\n",
    "                                              returnCountName=\"total_reviews\")\n",
    "print(\"Top Product By Review:\")\n",
    "dfProductReviewCounts.show(1)\n",
    "print(\"Top 10 Products by Reviews:\")\n",
    "dfProductReviewCounts.show(10)\n",
    "\n",
    "product_review_median=int(dfProductReviewCounts.approxQuantile(\"total_reviews\", [0.50], 0)[0])\n",
    "print(\"median number of {0} reviews per product\".format(product_review_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Filtering Unwanted Data\n",
    "\n",
    "filter reviews based on length, reviewer and product feature. In particular, the following reviews should be removed:\n",
    "\n",
    "* reviews with less than two sentences in the review body.\n",
    "* reviews published by users with less than median number of reviews published\n",
    "* reviews from products with less than median number of reviews received\n",
    "\n",
    "NOTE: Sentence Segmentation Using: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 99977\n",
      "number of rows post filter: 28955\n"
     ]
    }
   ],
   "source": [
    "# reviews with less than 2 sentences in review_body\n",
    "# convert to RDD and carry out a filter to remove rows with less than 2 sentences \n",
    "\n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "dfFilteredReviews = dfFilteredReviews.filter(helper.FilterSentences('review_body'))\n",
    "\n",
    "#dfFilteredReviews.show(1)\n",
    "dfFilteredReviews.cache()\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 28955\n",
      "number of rows post filter: 28955\n"
     ]
    }
   ],
   "source": [
    "# user review filter \n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\")\n",
    "dfFilteredReviews = dfFilteredReviews \\\n",
    "    .withColumn(\"review_count\", count(\"review_id\") \\\n",
    "    .over(window)) \\\n",
    "    .filter(col(\"review_count\") >= user_review_median) \\\n",
    "    .drop(\"review_count\")\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 28955\n",
      "number of rows post filter: 28955\n"
     ]
    }
   ],
   "source": [
    "# product review filter\n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "window = Window.partitionBy(\"product_id\")\n",
    "dfFilteredReviews = dfFilteredReviews \\\n",
    "    .withColumn(\"review_count\", count(\"review_id\") \\\n",
    "    .over(window)) \\\n",
    "    .filter(col(\"review_count\") >= product_review_median) \\\n",
    "    .drop(\"review_count\")\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Number of Rows before cleanup: 100000\n",
      "Number of rows after all filters applied: 28955\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Number of Rows before cleanup: {0}\".format(dfProductReview.count()))\n",
    "print(\"Number of rows after all filters applied: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, review_id: string, product_id: string, product_title: string, product_category: string, star_rating: int, helpful_votes: int, total_votes: int, review_headline: string, review_body: string, review_date: date]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFilteredReviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After filtering out the above, find out:\n",
    "\n",
    "* top 10 users ranked by median number of sentences in the reviews they have published\n",
    "* top 10 products ranked by median number of sentences in the reviews they have received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|median_sents|\n",
      "+-----------+------------+\n",
      "|   49745257|         241|\n",
      "|   49754397|         239|\n",
      "|   14678937|         155|\n",
      "|   22434772|         127|\n",
      "|   20894201|         103|\n",
      "|   25007515|          87|\n",
      "|    1459729|          82|\n",
      "|   51979520|          82|\n",
      "|   49758023|          79|\n",
      "|   34564717|          77|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 users ranked by median number of sentences in the reviews they have published\n",
    "dfTop10UsersBySents = helper.getTopBySentMedian(dfRecords=dfFilteredReviews, \\\n",
    "                                                partitionBy=\"customer_id\", \\\n",
    "                                                textCol=\"review_body\",\\\n",
    "                                                medianColName=\"median_sents\",n=10)\n",
    "dfTop10UsersBySents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|median_sents|\n",
      "+----------+------------+\n",
      "|B003ZUCXP2|         217|\n",
      "|B010SOIC5C|         194|\n",
      "|B00Q9H9GBM|         151|\n",
      "|B003UUQ7OK|         150|\n",
      "|B000007WPE|         148|\n",
      "|B00FY3X5GO|         146|\n",
      "|B00FG1EVUS|         145|\n",
      "|B003XD03DU|         134|\n",
      "|B012IV1E62|         134|\n",
      "|B00GG3JEU2|         134|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 products ranked by median number of sentences in the reviews they have received\n",
    "dfTop10ProductsBySents = helper.getTopBySentMedian(dfRecords=dfFilteredReviews, \\\n",
    "                                                   partitionBy=\"product_id\", \\\n",
    "                                                   textCol=\"review_body\", \\\n",
    "                                                   medianColName=\"median_sents\",n=10)\n",
    "\n",
    "dfTop10ProductsBySents.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned and filtered dataframe to file system \n",
    "\n",
    "#dfFilteredReviews.coalesce(1).write.format(\"parquet\") \\\n",
    "#    .option(\"header\", \"true\").saveAsTable('filteredReviews',mode=\"overwrite\")\n",
    "dfFilteredReviews.coalesce(1).write.csv(\"../output\",mode=\"overwrite\",header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 Similarity analysis with Sentence Embedding\n",
    "\n",
    "perform similarity analysis on the review sentences. The analysis involves segmenting review body into multiple sentences; encoding each sentence as vector so that the distance between pair of sentences can be computed.\n",
    "\n",
    "### Positive vs. Negative Reviews\n",
    "\n",
    "* pick a product from the top 10 products in stage 1\n",
    "* Create a positive and negative class of reviews using the rating \n",
    "    - Positive Class - rate >=4 \n",
    "    - Negative Class - rate <= 2\n",
    "    - for each review, extracting the review body part and segment it into multiple sentences.\n",
    "    - encode the sentences using google universal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, when, isnull\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "filtered_data = \"../output/part-*.csv\"\n",
    "dfBaseDataset = spark.read.csv(filtered_data,header=True,sep=\"\\t\",schema=aws_product_review_schema_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product from top 10 by review number \n",
    "base_product_id = \"B00MIA0KGY\"\n",
    "\n",
    "dfSelectedProduct = dfBaseDataset.where((col(\"product_id\") == base_product_id))\n",
    "dfPositiveClass = dfBaseDataset.where((col(\"product_id\") == base_product_id) & (col(\"star_rating\") >= 4))\n",
    "dfNegativeClass = dfBaseDataset.where((col(\"product_id\") == base_product_id) & (col(\"star_rating\") <= 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28955"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBaseDataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews from B00MIA0KGY: 14\n",
      "number of positives reviews from B00MIA0KGY: 11\n",
      "number of negatives reviews from B00MIA0KGY: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"number of reviews from {0}: {1}\".format(base_product_id, dfSelectedProduct.count()))\n",
    "print(\"number of positives reviews from {0}: {1}\".format(base_product_id,dfPositiveClass.count()))\n",
    "print(\"number of negatives reviews from {0}: {1}\".format(base_product_id,dfNegativeClass.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the sentences - similar to flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences from positive reviews: 75\n"
     ]
    }
   ],
   "source": [
    "# for each review, extract the review body part and segment it into multiple sentences\n",
    "# extract the positive sentences\n",
    "dfPosSents = dfPositiveClass.select(\"review_id\",\"review_body\") \\\n",
    "    .withColumn(\"sentences\", helper.GenerateSentences(\"review_body\")) \\\n",
    "    .select(\"review_id\", F.explode_outer(\"sentences\").alias(\"sentence\"))\n",
    "\n",
    "print(\"number of sentences from positive reviews: {0}\".format(dfPosSents.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences from negative reviews: 4\n",
      "+--------------+--------------------+\n",
      "|     review_id|            sentence|\n",
      "+--------------+--------------------+\n",
      "|R3VTZ9CGH2A2BZ|I thought I would...|\n",
      "|R3VTZ9CGH2A2BZ|Didn't enjoy a si...|\n",
      "|R3VTZ9CGH2A2BZ|Her Duets album w...|\n",
      "|R3VTZ9CGH2A2BZ|I'll be REselling...|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the negative sentences\n",
    "dfNegSents = dfNegativeClass.select(\"review_id\",\"review_body\") \\\n",
    "    .withColumn(\"sentences\", helper.GenerateSentences(\"review_body\")) \\\n",
    "    .select(\"review_id\", F.explode_outer(\"sentences\").alias(\"sentence\"))\n",
    "\n",
    "print(\"number of sentences from negative reviews: {0}\".format(dfNegSents.count()))\n",
    "\n",
    "dfNegSents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the sentences - google universal encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0511 18:10:11.913211 140735894979520 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'customHelpers' from '/Users/luisedumuller/Documents/Studies/MDS/CloudComputing/Assignment2/COMP5349_AmazonProductReviewAnalysis/notebooks/customHelpers.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType,BooleanType,DateType,ArrayType\n",
    "\n",
    "reload(helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the data for embedding: first step selecting the sentence column from the \n",
    "# data frames sentences. not sure why it doesn't work if don't use '.limit()' even if\n",
    "# dataframe is short \n",
    "\n",
    "pos_rev_text = dfPosSents.limit(10000).select('sentence')\n",
    "#second: convert the dataframe to rdd pipeline\n",
    "pos_rev_clean_text_rdd = pos_rev_text.rdd.map(lambda row: str(row[0])) \\\n",
    "                    .filter(lambda data: data is not None).cache()\n",
    "\n",
    "neg_rev_text = dfNegSents.limit(10000).select('sentence')\n",
    "#second: convert the dataframe to rdd pipeline\n",
    "neg_rev_clean_text_rdd = neg_rev_text.rdd.map(lambda row: str(row[0])) \\\n",
    "                    .filter(lambda data: data is not None).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function from lab 9\n",
    "def review_embed(rev_text_partition):\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "    rev_text_list = [text for text in rev_text_partition]\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(rev_text_list))\n",
    "    \n",
    "    return message_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the embedding from sentences\n",
    "positive_review_embedding = pos_rev_clean_text_rdd.mapPartitions(review_embed).cache()\n",
    "negative_review_embedding = neg_rev_clean_text_rdd.mapPartitions(review_embed).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Class Similarity\n",
    "\n",
    "We want to find out if sentences in the same category are closely related with each other. The closeness is measured by average distance between points in the class. In our case, point refers to the sentence encoding and pair-wise distance is measured by Cosine distance. Cosine distance is computed as â1 â CosineSimilarityâ. It has a value between 0 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.28 ms, sys: 8.98 ms, total: 15.3 ms\n",
      "Wall time: 22.8 s\n",
      "Average cosine distance between positive reviews: 0.6777541090667892\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "reload(helper)\n",
    "# create an rdd with PySparkVector and zip it with an index.\n",
    "neg_vect_index = negative_review_embedding.map(Vectors.dense).zipWithIndex()\n",
    "\n",
    "#crete a cartesian combination of the vectors, index pair \n",
    "#eg: ((vect0,0), (vect1,1)), ((vect1,1), (vect2,2)), etc... \n",
    "neg_distances = neg_vect_index.cartesian(neg_vect_index) \\\n",
    "                       .map(helper.CosineDistance) \\\n",
    "                       .filter(lambda x: x[2] != 0.0)\n",
    "\n",
    "%time neg_result = neg_distances.collect()\n",
    "neg_avg = sum(i[2] for i in neg_result)/len(neg_result)\n",
    "\n",
    "print(\"Average cosine distance between positive reviews: {0}\".format(neg_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.38 ms, sys: 5.05 ms, total: 12.4 ms\n",
      "Wall time: 24.2 s\n",
      "Average cosine distance between positive reviews: 0.6657311997824882\n"
     ]
    }
   ],
   "source": [
    "pos_vect_index = positive_review_embedding.map(Vectors.dense).zipWithIndex()\n",
    "\n",
    "pos_distances = pos_vect_index.cartesian(pos_vect_index) \\\n",
    "                       .map(helper.CosineDistance) \\\n",
    "                       .filter(lambda x: x[2] != 0.0)\n",
    "\n",
    "%time pos_result = pos_distances.collect()\n",
    "pos_avg = sum(i[2] for i in pos_result)/len(pos_result)\n",
    "\n",
    "print(\"Average cosine distance between positive reviews: {0}\".format(pos_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Center Sentences\n",
    "\n",
    "Find out the class center and its 10 closest neighbours for positive and negative class respectively. We define class center as the point that has the smallest average distance to other points in the class. Again in this case point refers to the sentence encoding and pair-wise distance are measured by Cosine distance.\n",
    "The result should show the text of the center sentence, the review id it belongs to and its 10 closest neighbouring sentences text and their respective review id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg\n",
    "\n",
    "aws_distances_schema = StructType([\n",
    "    StructField(\"id_origin\", IntegerType(), True),\n",
    "    StructField(\"id_dest\", IntegerType(), True),\n",
    "    StructField(\"distances\",FloatType(), True)])\n",
    "\n",
    "dfNegDistances = spark.createDataFrame(neg_distances,aws_distances_schema)\n",
    "dfPosDistances = spark.createDataFrame(pos_distances,aws_distances_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center sentence from Positive class with minimum average distance\n",
      "+---------+------------------+\n",
      "|id_origin|    avg(distances)|\n",
      "+---------+------------------+\n",
      "|       13|0.5181285652357179|\n",
      "+---------+------------------+\n",
      "\n",
      "Ten closest sentences from positive class centre:\n",
      "+---------+-------+----------+\n",
      "|id_origin|id_dest| distances|\n",
      "+---------+-------+----------+\n",
      "|       13|     14|0.17537284|\n",
      "|       13|     53|0.20937957|\n",
      "|       13|     62|0.26685855|\n",
      "|       13|     21|0.28175992|\n",
      "|       13|     31|0.30157587|\n",
      "|       13|      3|0.30667576|\n",
      "|       13|     57|0.31021476|\n",
      "|       13|     29|0.31616253|\n",
      "|       13|      8|0.32270247|\n",
      "|       13|     27|0.32730842|\n",
      "+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPosCentre = dfPosDistances.groupBy(\"id_origin\").agg(avg(\"distances\")) \\\n",
    "                       .orderBy(\"avg(distances)\") \\\n",
    "                       .limit(1)\n",
    "posCenterid  = dfPosCentre.collect()[0][0]\n",
    "\n",
    "print(\"Center sentence from Positive class with minimum average distance\")\n",
    "dfPosCentre.show()\n",
    "\n",
    "dfPosClosest = dfPosDistances.where(col(\"id_origin\") == posCenterid) \\\n",
    "                             .orderBy(\"distances\") \\\n",
    "                             .limit(10)\n",
    "\n",
    "print(\"Ten closest sentences from positive class centre:\")\n",
    "\n",
    "dfPosClosest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center sentence from Negative class with minimum average distance\n",
      "+---------+------------------+\n",
      "|id_origin|    avg(distances)|\n",
      "+---------+------------------+\n",
      "|        0|0.5532332261403402|\n",
      "+---------+------------------+\n",
      "\n",
      "Ten closest sentences from negative class centre:\n",
      "+---------+-------+----------+\n",
      "|id_origin|id_dest| distances|\n",
      "+---------+-------+----------+\n",
      "|        0|      2| 0.3353635|\n",
      "|        0|      3|0.59003365|\n",
      "|        0|      1| 0.7343025|\n",
      "+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNegCentre = dfNegDistances.groupBy(\"id_origin\").agg(avg(\"distances\")) \\\n",
    "                       .orderBy(\"avg(distances)\") \\\n",
    "                       .limit(1)\n",
    "\n",
    "negCenterid  = dfNegCentre.collect()[0][0]\n",
    "\n",
    "print(\"Center sentence from Negative class with minimum average distance\")\n",
    "dfNegCentre.show()\n",
    "\n",
    "dfNegClosest = dfNegDistances.where(col(\"id_origin\") == negCenterid) \\\n",
    "                             .orderBy(\"distances\") \\\n",
    "                             .limit(10)\n",
    "\n",
    "print(\"Ten closest sentences from negative class centre:\")\n",
    "\n",
    "dfNegClosest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
