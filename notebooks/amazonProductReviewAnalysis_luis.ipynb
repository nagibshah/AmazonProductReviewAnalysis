{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'customHelpers' from '/Users/luisedumuller/Documents/Studies/MDS/CloudComputing/Assignment2/COMP5349_AmazonProductReviewAnalysis/notebooks/customHelpers.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# library imports \n",
    "import sys\n",
    "from importlib import reload\n",
    "#import findspark\n",
    "import customHelpers as helper\n",
    "#findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType,BooleanType,DateType\n",
    "\n",
    "reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the session \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Amazon Product Review Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset \n",
    "\n",
    "| Column | Description | \n",
    "| :--- | :--- |\n",
    "| marketplace | 2 letter country code of the marketplace where the review was written. |\n",
    "| customer_id | Random identifier that can be used to aggregate reviews written by a single author. |\n",
    "| review_id | The unique ID of the review. |\n",
    "| product_id | The unique Product ID the review pertains to. In the multilingual dataset the reviews for the same product in different countries can be grouped by the same product_id. | \n",
    "| product_parent | Random identifier that can be used to aggregate reviews for the same product. |\n",
    "| product_title | Title of the product. | \n",
    "| product_category | Broad product category that can be used to group reviews (also used to group the dataset into  coherent parts). | \n",
    "| star_rating | the 1-5 star rating of the review. | \n",
    "| helpful_votes | Number of helpful votes. | \n",
    "| total_votes | Number of total votes the review received. | \n",
    "| vine | Review was written as part of the Vine program. |\n",
    "| verified_purchase | The review is on a verified purchase. |\n",
    "| review_headline | The title of the review. |\n",
    "| review_body | The review text. |\n",
    "| review_date | The date the review was written | \n",
    "\n",
    "\n",
    "DATA FORMAT\n",
    "Tab ('\\t') separated text file, without quote or escape characters.\n",
    "First line in each file is header; 1 line corresponds to 1 record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 ms, sys: 956 µs, total: 2.93 ms\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "# load the data set \n",
    "#review_data = '../data/sample_us.tsv'\n",
    "# actual data load - PERFORMANCE WARNING ON LOCAL MACHINE\n",
    "review_data = '../data/amazon_reviews_us_Music_v1_00.tsv'\n",
    "\n",
    "aws_product_review_schema = StructType([\n",
    "    StructField(\"marketplace\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\",StringType(),True),\n",
    "    StructField(\"product_parent\",StringType(),False),\n",
    "    StructField(\"product_title\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"star_rating\", IntegerType(), False),\n",
    "    StructField(\"helpful_votes\",IntegerType(),False),\n",
    "    StructField(\"total_votes\", IntegerType(), False),\n",
    "    StructField(\"vine\",StringType(),False),\n",
    "    StructField(\"verified_purchase\", StringType(), False),\n",
    "    StructField(\"review_headline\", StringType(), False),\n",
    "    StructField(\"review_body\", StringType(), False),\n",
    "    StructField(\"review_date\",DateType(),False)])\n",
    "\n",
    "aws_product_review_schema_limited = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\",StringType(),True),\n",
    "    StructField(\"product_title\", StringType(), False),\n",
    "    StructField(\"product_category\", StringType(), False),\n",
    "    StructField(\"star_rating\", IntegerType(), False),\n",
    "    StructField(\"helpful_votes\",IntegerType(),False),\n",
    "    StructField(\"total_votes\", IntegerType(), False),\n",
    "    StructField(\"review_headline\", StringType(), False),\n",
    "    StructField(\"review_body\", StringType(), False),\n",
    "    StructField(\"review_date\",DateType(),False)])\n",
    "\n",
    "%time awsProductReview_raw_data = spark.read.csv(review_data,header=True,sep=\"\\t\",schema=aws_product_review_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4751577\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# when testing in local machine only \n",
    "print(awsProductReview_raw_data.count())\n",
    "# limit to 1 mil\n",
    "awsProductReview_raw_data = awsProductReview_raw_data.limit(100000)\n",
    "print(awsProductReview_raw_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProductReview = awsProductReview_raw_data.drop('vine').drop('verified_purchase') \\\n",
    "                    .drop('product_parent').drop('marketplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 100000\n",
      "number of rows after filter: 99977\n"
     ]
    }
   ],
   "source": [
    "# remove rows with no review text \n",
    "print(\"number of rows before filter: {0}\".format(dfProductReview.count()))\n",
    "dfFilteredReviews = dfProductReview.na.drop(subset=[\"review_body\"])\n",
    "print(\"number of rows after filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Overall statistics\n",
    "\n",
    "### Produce overall summary statistics of the data set, in particular,\n",
    "\n",
    "* the total number of reviews\n",
    "* the number of unique users\n",
    "* the number of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, when, isnull\n",
    "\n",
    "dfOverallStats = dfFilteredReviews.agg(countDistinct(\"customer_id\").alias(\"unique_customers\"), \\\n",
    "                    countDistinct(\"product_id\").alias(\"unique_products\"), \\\n",
    "                    count(col=\"review_id\").alias(\"total_reviews\")) \\\n",
    "\n",
    "#dfOverallStats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+---------------+-----------+-----------+\n",
      "|customer_id|review_id|product_id|product_title|product_category|star_rating|helpful_votes|total_votes|review_headline|review_body|review_date|\n",
      "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+---------------+-----------+-----------+\n",
      "|          0|        0|         0|            0|               0|          0|            0|          0|              0|          0|          0|\n",
      "+-----------+---------+----------+-------------+----------------+-----------+-------------+-----------+---------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHECK IN ALL THE COLUMNS IF THERE IS ANY NULL VALUE.\n",
    "dfFilteredReviews.select([count(when(isnull(column), column)).alias(column) \\\n",
    "                        for column in dfProductReview.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For user-review distribution, you are asked to find out:\n",
    "\n",
    "* the largest number of reviews published by a single user\n",
    "* the top 10 users ranked by the number of reviews they publish\n",
    "* the median number of reviews published by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Reviewer:\n",
      "+-----------+-------------+\n",
      "|customer_id|total_reviews|\n",
      "+-----------+-------------+\n",
      "|   15536614|          373|\n",
      "+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Top 10 Reviewers:\n",
      "+-----------+-------------+\n",
      "|customer_id|total_reviews|\n",
      "+-----------+-------------+\n",
      "|   15536614|          373|\n",
      "|   38214553|          226|\n",
      "|    4276914|          222|\n",
      "|    8342883|          194|\n",
      "|    5291529|          190|\n",
      "|   13634768|          180|\n",
      "|    2112938|          126|\n",
      "|   38229524|           98|\n",
      "|   48046800|           97|\n",
      "|   22716027|           92|\n",
      "+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "median number of 1.0 reviews published by user\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "dfUserReviewCounts = helper.distributionStats(dfRecords=dfFilteredReviews.select(\"customer_id\", \"review_id\"), \\\n",
    "                                              partitionBy=\"customer_id\",countBy=\"review_id\", \\\n",
    "                                              returnCountName=\"total_reviews\")\n",
    "print(\"Top Reviewer:\")\n",
    "dfUserReviewCounts.show(1)\n",
    "print(\"Top 10 Reviewers:\")\n",
    "dfUserReviewCounts.show(10)\n",
    "\n",
    "user_review_median=dfUserReviewCounts.approxQuantile(\"total_reviews\", [0.50], 0)[0]\n",
    "print(\"median number of {0} reviews published by user\".format(user_review_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For product-review distribution, you are asked to find out:\n",
    "    \n",
    "* the largest number of reviews written for a single product\n",
    "* the top 10 products ranked by the number of reviews they have\n",
    "* the median number of reviews a product has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Product By Review:\n",
      "+----------+-------------+\n",
      "|product_id|total_reviews|\n",
      "+----------+-------------+\n",
      "|B00VXGTJMU|          256|\n",
      "+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "Top 10 Products by Reviews:\n",
      "+----------+-------------+\n",
      "|product_id|total_reviews|\n",
      "+----------+-------------+\n",
      "|B00VXGTJMU|          256|\n",
      "|B00VMRJPCE|          255|\n",
      "|B00UCFVIDQ|          205|\n",
      "|B00VMQK37Q|          170|\n",
      "|B00ZQUP38S|          158|\n",
      "|B00WE2SMKC|          139|\n",
      "|B00WSOWR0M|          120|\n",
      "|B00MRHANNI|          116|\n",
      "|B00VTBBEL8|          115|\n",
      "|B00XJJAWES|          112|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "median number of 1 reviews per product\n"
     ]
    }
   ],
   "source": [
    "dfProductReviewCounts = helper.distributionStats(dfRecords=dfProductReview.select(\"product_id\", \"review_id\"), \\\n",
    "                                              partitionBy=\"product_id\",countBy=\"review_id\", \\\n",
    "                                              returnCountName=\"total_reviews\")\n",
    "print(\"Top Product By Review:\")\n",
    "dfProductReviewCounts.show(1)\n",
    "print(\"Top 10 Products by Reviews:\")\n",
    "dfProductReviewCounts.show(10)\n",
    "\n",
    "product_review_median=int(dfProductReviewCounts.approxQuantile(\"total_reviews\", [0.50], 0)[0])\n",
    "print(\"median number of {0} reviews per product\".format(product_review_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Filtering Unwanted Data\n",
    "\n",
    "filter reviews based on length, reviewer and product feature. In particular, the following reviews should be removed:\n",
    "\n",
    "* reviews with less than two sentences in the review body.\n",
    "* reviews published by users with less than median number of reviews published\n",
    "* reviews from products with less than median number of reviews received\n",
    "\n",
    "NOTE: Sentence Segmentation Using: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 99977\n",
      "number of rows post filter: 28955\n"
     ]
    }
   ],
   "source": [
    "# reviews with less than 2 sentences in review_body\n",
    "# convert to RDD and carry out a filter to remove rows with less than 2 sentences \n",
    "\n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "dfFilteredReviews = dfFilteredReviews.filter(helper.FilterSentences('review_body'))\n",
    "\n",
    "#dfFilteredReviews.show(1)\n",
    "dfFilteredReviews.cache()\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 28955\n",
      "number of rows post filter: 28955\n"
     ]
    }
   ],
   "source": [
    "# user review filter \n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "window = Window.partitionBy(\"customer_id\")\n",
    "dfFilteredReviews = dfFilteredReviews \\\n",
    "    .withColumn(\"review_count\", count(\"review_id\") \\\n",
    "    .over(window)) \\\n",
    "    .filter(col(\"review_count\") >= user_review_median) \\\n",
    "    .drop(\"review_count\")\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows before filter: 28955\n",
      "number of rows post filter: 28955\n"
     ]
    }
   ],
   "source": [
    "# product review filter\n",
    "print(\"number of rows before filter: {0}\".format(dfFilteredReviews.count()))\n",
    "\n",
    "window = Window.partitionBy(\"product_id\")\n",
    "dfFilteredReviews = dfFilteredReviews \\\n",
    "    .withColumn(\"review_count\", count(\"review_id\") \\\n",
    "    .over(window)) \\\n",
    "    .filter(col(\"review_count\") >= product_review_median) \\\n",
    "    .drop(\"review_count\")\n",
    "\n",
    "print(\"number of rows post filter: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Number of Rows before cleanup: 100000\n",
      "Number of rows after all filters applied: 28955\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Number of Rows before cleanup: {0}\".format(dfProductReview.count()))\n",
    "print(\"Number of rows after all filters applied: {0}\".format(dfFilteredReviews.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, review_id: string, product_id: string, product_title: string, product_category: string, star_rating: int, helpful_votes: int, total_votes: int, review_headline: string, review_body: string, review_date: date]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFilteredReviews.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After filtering out the above, find out:\n",
    "\n",
    "* top 10 users ranked by median number of sentences in the reviews they have published\n",
    "* top 10 products ranked by median number of sentences in the reviews they have received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|median_sents|\n",
      "+-----------+------------+\n",
      "|   49745257|         241|\n",
      "|   49754397|         239|\n",
      "|   14678937|         155|\n",
      "|   22434772|         127|\n",
      "|   20894201|         103|\n",
      "|   25007515|          87|\n",
      "|    1459729|          82|\n",
      "|   51979520|          82|\n",
      "|   49758023|          79|\n",
      "|   34564717|          77|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 users ranked by median number of sentences in the reviews they have published\n",
    "dfTop10UsersBySents = helper.getTopBySentMedian(dfRecords=dfFilteredReviews, \\\n",
    "                                                partitionBy=\"customer_id\", \\\n",
    "                                                textCol=\"review_body\",\\\n",
    "                                                medianColName=\"median_sents\",n=10)\n",
    "dfTop10UsersBySents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|product_id|median_sents|\n",
      "+----------+------------+\n",
      "|B003ZUCXP2|         217|\n",
      "|B010SOIC5C|         194|\n",
      "|B00Q9H9GBM|         151|\n",
      "|B003UUQ7OK|         150|\n",
      "|B000007WPE|         148|\n",
      "|B00FY3X5GO|         146|\n",
      "|B00FG1EVUS|         145|\n",
      "|B003XD03DU|         134|\n",
      "|B012IV1E62|         134|\n",
      "|B00GG3JEU2|         134|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 products ranked by median number of sentences in the reviews they have received\n",
    "dfTop10ProductsBySents = helper.getTopBySentMedian(dfRecords=dfFilteredReviews, \\\n",
    "                                                   partitionBy=\"product_id\", \\\n",
    "                                                   textCol=\"review_body\", \\\n",
    "                                                   medianColName=\"median_sents\",n=10)\n",
    "\n",
    "dfTop10ProductsBySents.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned and filtered dataframe to file system \n",
    "\n",
    "#dfFilteredReviews.coalesce(1).write.format(\"parquet\") \\\n",
    "#    .option(\"header\", \"true\").saveAsTable('filteredReviews',mode=\"overwrite\")\n",
    "dfFilteredReviews.coalesce(1).write.csv(\"../output\",mode=\"overwrite\",header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 Similarity analysis with Sentence Embedding\n",
    "\n",
    "perform similarity analysis on the review sentences. The analysis involves segmenting review body into multiple sentences; encoding each sentence as vector so that the distance between pair of sentences can be computed.\n",
    "\n",
    "### Positive vs. Negative Reviews\n",
    "\n",
    "* pick a product from the top 10 products in stage 1\n",
    "* Create a positive and negative class of reviews using the rating \n",
    "    - Positive Class - rate >=4 \n",
    "    - Negative Class - rate <= 2\n",
    "    - for each review, extracting the review body part and segment it into multiple sentences.\n",
    "    - encode the sentences using google universal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, when, isnull\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "filtered_data = \"../output/part-*.csv\"\n",
    "dfBaseDataset = spark.read.csv(filtered_data,header=True,sep=\"\\t\",schema=aws_product_review_schema_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product from top 10 by review number \n",
    "base_product_id = \"B00MIA0KGY\"\n",
    "\n",
    "dfSelectedProduct = dfBaseDataset.where((col(\"product_id\") == base_product_id))\n",
    "dfPositiveClass = dfBaseDataset.where((col(\"product_id\") == base_product_id) & (col(\"star_rating\") >= 4))\n",
    "dfNegativeClass = dfBaseDataset.where((col(\"product_id\") == base_product_id) & (col(\"star_rating\") <= 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329468"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBaseDataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews from B00MIA0KGY: 638\n",
      "number of positives reviews from B00MIA0KGY: 549\n",
      "number of negatives reviews from B00MIA0KGY: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"number of reviews from {0}: {1}\".format(base_product_id, dfSelectedProduct.count()))\n",
    "print(\"number of positives reviews from {0}: {1}\".format(base_product_id,dfPositiveClass.count()))\n",
    "print(\"number of negatives reviews from {0}: {1}\".format(base_product_id,dfNegativeClass.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the sentences - similar to flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences from positive reviews: 3018\n"
     ]
    }
   ],
   "source": [
    "# for each review, extract the review body part and segment it into multiple sentences\n",
    "# extract the positive sentences\n",
    "dfPosSents = dfPositiveClass.select(\"review_id\",\"review_body\") \\\n",
    "    .withColumn(\"sentences\", helper.GenerateSentences(\"review_body\")) \\\n",
    "    .select(\"review_id\", F.explode_outer(\"sentences\").alias(\"sentence\")) \\\n",
    "    .na.drop(subset=[\"sentence\"])\n",
    "\n",
    "print(\"number of sentences from positive reviews: {0}\".format(dfPosSents.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences from negative reviews: 314\n",
      "+--------------+--------------------+\n",
      "|     review_id|            sentence|\n",
      "+--------------+--------------------+\n",
      "|R2P2KVK3GRJBHP|     Love her voice!|\n",
      "|R2P2KVK3GRJBHP|Few surprises, sh...|\n",
      "|R2P2KVK3GRJBHP|Pairing with a pa...|\n",
      "|R2P2KVK3GRJBHP|Her son has a bea...|\n",
      "|R1HY9W9AU5S4WB|   So disappointed!!|\n",
      "|R1HY9W9AU5S4WB|!why did she do it??|\n",
      "|R1HY9W9AU5S4WB|                   ?|\n",
      "| RT90E78LWTMTC|Only song I liked...|\n",
      "| RT90E78LWTMTC|     It has feeling.|\n",
      "| RT90E78LWTMTC|In all the others...|\n",
      "|R1TXTNSLRNGDYI|If you think this...|\n",
      "|R1TXTNSLRNGDYI|Mostly  it's the ...|\n",
      "|R1TXTNSLRNGDYI|Give the guys a c...|\n",
      "| R8WGLIPJJR975|       Love Barbara!|\n",
      "| R8WGLIPJJR975|Own practically e...|\n",
      "| R8WGLIPJJR975|         Hated this!|\n",
      "| R8WGLIPJJR975|could not even li...|\n",
      "| R8WGLIPJJR975|Her singing, for ...|\n",
      "| R8WGLIPJJR975|her partners, wit...|\n",
      "| R8WGLIPJJR975|A number of the a...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract the negative sentences\n",
    "dfNegSents = dfNegativeClass.select(\"review_id\",\"review_body\") \\\n",
    "    .withColumn(\"sentences\", helper.GenerateSentences(\"review_body\")) \\\n",
    "    .select(\"review_id\", F.explode_outer(\"sentences\").alias(\"sentence\")) \\\n",
    "    .na.drop(subset=[\"sentence\"])\n",
    "\n",
    "print(\"number of sentences from negative reviews: {0}\".format(dfNegSents.count()))\n",
    "\n",
    "dfNegSents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the sentences - google universal encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'customHelpers' from '/Users/luisedumuller/Documents/Studies/MDS/CloudComputing/Assignment2/COMP5349_AmazonProductReviewAnalysis/notebooks/customHelpers.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType,BooleanType,DateType,ArrayType\n",
    "\n",
    "reload(helper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|     review_id|             vectors|\n",
      "+--------------+--------------------+\n",
      "|R33AKM6TMGP62U|[-0.0016616370994...|\n",
      "|R33AKM6TMGP62U|[0.00872488133609...|\n",
      "|R33AKM6TMGP62U|[-0.0347822681069...|\n",
      "|R338L3ESXHT0XJ|[0.01993116922676...|\n",
      "|R338L3ESXHT0XJ|[-0.0055921743623...|\n",
      "|R338L3ESXHT0XJ|[0.02325326018035...|\n",
      "|R338L3ESXHT0XJ|[-0.0637154728174...|\n",
      "|R338L3ESXHT0XJ|[-0.0039158728905...|\n",
      "|R338L3ESXHT0XJ|[0.02630591951310...|\n",
      "|R338L3ESXHT0XJ|[-0.0169928558170...|\n",
      "|R1YIG5CA2CR3FC|[-0.0058144275099...|\n",
      "|R1YIG5CA2CR3FC|[0.01800153031945...|\n",
      "|R1YIG5CA2CR3FC|[-0.0625827088952...|\n",
      "|R23WXSCWPOBERM|[0.00281153293326...|\n",
      "|R23WXSCWPOBERM|[0.01098206080496...|\n",
      "|R23WXSCWPOBERM|[1.21090859465766...|\n",
      "|R337RW8HCJLL7H|[0.02948248200118...|\n",
      "|R337RW8HCJLL7H|[-0.0196939110755...|\n",
      "|R337RW8HCJLL7H|[0.06208934262394...|\n",
      "|R337RW8HCJLL7H|[-0.0235339216887...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### old Luis version ### --------------------------------------------------------------\n",
    "\n",
    "# Processing the data for embedding: first step selecting the sentence column from the \n",
    "# data frames sentences. not sure why it doesn't work if don't use '.limit()' even if\n",
    "# dataframe is short \n",
    "\n",
    "#pos_rev_text = dfPosSents.limit(10000).select('sentence')\n",
    "#second: convert the dataframe to rdd pipeline\n",
    "#pos_rev_clean_text_rdd = pos_rev_text.rdd.map(lambda row: str(row[0])) \\\n",
    "#                    .filter(lambda data: data is not None).cache()\n",
    "\n",
    "#neg_rev_text = dfNegSents.limit(10000).select('sentence')\n",
    "#second: convert the dataframe to rdd pipeline\n",
    "#neg_rev_clean_text_rdd = neg_rev_text.rdd.map(lambda row: str(row[0])) \\\n",
    "#                    .filter(lambda data: data is not None).cache()\n",
    "\n",
    "### old Luis version ### --------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## Nagib version ### ------------------------------------------------------------------ \n",
    "\n",
    "##### bug with tensorflow if no limit is set\n",
    "# bypass by setting df count as limit \n",
    "\n",
    "# get the negative embeddings + dense vectors \n",
    "#rddTemp = dfNegSents.limit(dfNegSents.count()).select(\"review_id\",\"sentence\").rdd.map(list) \\\n",
    "#            .mapPartitions(helper.vectorizeSents).cache()\n",
    "#dfNegSentsVectorised = spark.createDataFrame(rddTemp, helper.VECTOR_SCHEMA)\n",
    "\n",
    "#dfNegSentsVectorised.show()\n",
    "\n",
    "rddTemp = dfPosSents.limit(dfPosSents.count()).select(\"review_id\",\"sentence\").rdd.map(list) \\\n",
    "            .mapPartitions(helper.vectorizeSents).cache()\n",
    "dfPosSentsVectorised = spark.createDataFrame(rddTemp, helper.VECTOR_SCHEMA)\n",
    "\n",
    "dfPosSentsVectorised.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embedded and vectorised negative sentences 314\n",
      "Neg vector sample:\n",
      "+--------------+--------------------+\n",
      "|     review_id|             vectors|\n",
      "+--------------+--------------------+\n",
      "|R2P2KVK3GRJBHP|[-0.0506365746259...|\n",
      "+--------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some lines \n",
    "#print(\"Total embedded and vectorised positive sentences {0}\".format(dfPosSentsVectorised.count()))\n",
    "#print(\"Pos vector sample:\")\n",
    "#dfPosSentsVectorised.show(1)\n",
    "\n",
    "print(\"Total embedded and vectorised negative sentences {0}\".format(dfNegSentsVectorised.count()))\n",
    "print(\"Neg vector sample:\")\n",
    "dfNegSentsVectorised.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding old version - luis\n",
    "\n",
    "## function from lab 9\n",
    "#def review_embed(rev_text_partition):\n",
    "#    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "#    embed = hub.Module(module_url)\n",
    "#    rev_text_list = [text for text in rev_text_partition]\n",
    "#    with tf.Session() as session:\n",
    "#        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#        message_embeddings = session.run(embed(rev_text_list))\n",
    "    \n",
    "#    return message_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the embedding from sentences\n",
    "#positive_review_embedding = pos_rev_clean_text_rdd.mapPartitions(review_embed).cache()\n",
    "#negative_review_embedding = neg_rev_clean_text_rdd.mapPartitions(review_embed).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-Class Similarity\n",
    "\n",
    "We want to find out if sentences in the same category are closely related with each other. The closeness is measured by average distance between points in the class. In our case, point refers to the sentence encoding and pair-wise distance is measured by Cosine distance. Cosine distance is computed as “1 − CosineSimilarity”. It has a value between 0 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L2 norm\n",
      "+--------------+--------------------+--------------------+\n",
      "|     review_id|             vectors|        normFeatures|\n",
      "+--------------+--------------------+--------------------+\n",
      "|R2P2KVK3GRJBHP|[-0.0506365746259...|[-0.0506365802085...|\n",
      "|R2P2KVK3GRJBHP|[-0.0580368600785...|[-0.0580368585287...|\n",
      "|R2P2KVK3GRJBHP|[0.03794875741004...|[0.03794875709855...|\n",
      "|R2P2KVK3GRJBHP|[0.03115262463688...|[0.03115262578360...|\n",
      "|R1HY9W9AU5S4WB|[0.01509214658290...|[0.01509214560504...|\n",
      "+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"vectors\", outputCol=\"normFeatures\") # default uses L2 norm \n",
    "l2NegNormData = normalizer.transform(dfNegSentsVectorised)\n",
    "print(\"Normalized using L2 norm\")\n",
    "l2NegNormData.show(5)\n",
    "\n",
    "l2PosNormData = normalizer.transform(dfPosSentsVectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterNegMapData = l2NegNormData.select(\"review_id\",\"normFeatures\").rdd.zipWithIndex().cache()\n",
    "\n",
    "masterPosMapData = l2PosNormData.select(\"review_id\",\"normFeatures\").rdd.zipWithIndex().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "#mat = IndexedRowMatrix(masterMapData.map(lambda row: IndexedRow(row.review_id, row.normFeatures.toArray()))) \\\n",
    "#            .toBlockMatrix()\n",
    "\n",
    "def calc_distance(row):\n",
    "    rowid = row.index\n",
    "    distances = 1-row.vector\n",
    "    # now get the sums \n",
    "    totalDistance = distances.toArray().sum()\n",
    "    length = len(distances.toArray())\n",
    "    avg = totalDistance / length\n",
    "    return (rowid, avg)\n",
    "\n",
    "#matNeg = IndexedRowMatrix(masterNegMapData.map(lambda row: IndexedRow(row[1], (row[0][1]).toArray()))).toBlockMatrix()\n",
    "#dotNeg = matNeg.multiply(matNeg.transpose())\n",
    "#negCosines = dotNeg.toIndexedRowMatrix()\n",
    "#avgNegDistances = negCosines.rows.map(lambda row: calc_distance(row))\n",
    "\n",
    "matPos = IndexedRowMatrix(masterPosMapData.map(lambda row: IndexedRow(row[1], (row[0][1]).toArray()))).toBlockMatrix()\n",
    "dotPos = matPos.multiply(matPos.transpose())\n",
    "posCosines = dotPos.toIndexedRowMatrix()\n",
    "avgPosDistances = posCosines.rows.map(lambda row: calc_distance(row))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.94 ms, sys: 1.79 ms, total: 5.73 ms\n",
      "Wall time: 1.68 s\n",
      "0.6784501847286866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3018"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time x = avgPosDistances.collect()\n",
    "\n",
    "soma = 0\n",
    "count = 0\n",
    "for i in x :\n",
    "    soma += i[1]\n",
    "    count += 1\n",
    "print(soma/count)\n",
    "\n",
    "test = posCosines.rows.map(lambda row: row)\n",
    "ccc = test.collect()\n",
    "len(ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 ms, sys: 241 ms, total: 351 ms\n",
      "Wall time: 30.6 s\n",
      "Average cosine distance between positive reviews: 0.7156308870893071\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "reload(helper)\n",
    "# create an rdd with PySparkVector and zip it with an index.\n",
    "neg_vect_index = negative_review_embedding.map(Vectors.dense).zipWithIndex()\n",
    "\n",
    "#crete a cartesian combination of the vectors, index pair \n",
    "#eg: ((vect0,0), (vect1,1)), ((vect1,1), (vect2,2)), etc... \n",
    "neg_distances = neg_vect_index.cartesian(neg_vect_index) \\\n",
    "                       .map(helper.CosineDistance) \\\n",
    "                       .filter(lambda x: x[2] != 0.0)\n",
    "\n",
    "%time neg_result = neg_distances.collect()\n",
    "neg_avg = sum(i[2] for i in neg_result)/len(neg_result)\n",
    "\n",
    "print(\"Average cosine distance between positive reviews: {0}\".format(neg_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.38 ms, sys: 5.05 ms, total: 12.4 ms\n",
      "Wall time: 24.2 s\n",
      "Average cosine distance between positive reviews: 0.6657311997824882\n"
     ]
    }
   ],
   "source": [
    "pos_vect_index = positive_review_embedding.map(Vectors.dense).zipWithIndex()\n",
    "\n",
    "pos_distances = pos_vect_index.cartesian(pos_vect_index) \\\n",
    "                       .map(helper.CosineDistance) \\\n",
    "                       .filter(lambda x: x[2] != 0.0)\n",
    "\n",
    "%time pos_result = pos_distances.collect()\n",
    "pos_avg = sum(i[2] for i in pos_result)/len(pos_result)\n",
    "\n",
    "print(\"Average cosine distance between positive reviews: {0}\".format(pos_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Center Sentences\n",
    "\n",
    "Find out the class center and its 10 closest neighbours for positive and negative class respectively. We define class center as the point that has the smallest average distance to other points in the class. Again in this case point refers to the sentence encoding and pair-wise distance are measured by Cosine distance.\n",
    "The result should show the text of the center sentence, the review id it belongs to and its 10 closest neighbouring sentences text and their respective review id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, avg\n",
    "\n",
    "aws_distances_schema = StructType([\n",
    "    StructField(\"id_origin\", IntegerType(), True),\n",
    "    StructField(\"id_dest\", IntegerType(), True),\n",
    "    StructField(\"distances\",FloatType(), True)])\n",
    "\n",
    "dfNegDistances = spark.createDataFrame(neg_distances,aws_distances_schema)\n",
    "#dfPosDistances = spark.createDataFrame(pos_distances,aws_distances_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfPosDistances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3a102cc7ef9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#dfPosCentre.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdfPosClosest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfPosDistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id_origin\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mposCenterid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distances\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                              \u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfPosDistances' is not defined"
     ]
    }
   ],
   "source": [
    "#dfPosCentre = dfPosDistances.groupBy(\"id_origin\").agg(avg(\"distances\")) \\\n",
    "#                       .orderBy(\"avg(distances)\") \\\n",
    "#                       .limit(1)\n",
    "#posCenterid  = dfPosCentre.collect()[0][0]\n",
    "\n",
    "#print(\"Center sentence from Positive class with minimum average distance\")\n",
    "#dfPosCentre.show()\n",
    "\n",
    "#dfPosClosest = dfPosDistances.where(col(\"id_origin\") == posCenterid) \\\n",
    "#                             .orderBy(\"distances\") \\\n",
    "#                             .limit(10)\n",
    "\n",
    "#print(\"Ten closest sentences from positive class centre:\")\n",
    "\n",
    "#dfPosClosest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center sentence from Negative class with minimum average distance\n",
      "+---------+------------------+\n",
      "|id_origin|    avg(distances)|\n",
      "+---------+------------------+\n",
      "|      101|0.5902185768555528|\n",
      "+---------+------------------+\n",
      "\n",
      "Ten closest sentences from negative class centre:\n",
      "+---------+-------+----------+\n",
      "|id_origin|id_dest| distances|\n",
      "+---------+-------+----------+\n",
      "|      101|     37|0.18140683|\n",
      "|      101|     60|0.21687192|\n",
      "|      101|      7|0.25256255|\n",
      "|      101|    137|0.25421256|\n",
      "|      101|     91|0.25971147|\n",
      "|      101|    221|0.26143548|\n",
      "|      101|      3|0.27037904|\n",
      "|      101|    311|0.27135503|\n",
      "|      101|    180|0.27161464|\n",
      "|      101|    182| 0.2756444|\n",
      "+---------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNegCentre = dfNegDistances.groupBy(\"id_origin\").agg(avg(\"distances\")) \\\n",
    "                       .orderBy(\"avg(distances)\") \\\n",
    "                       .limit(1)\n",
    "\n",
    "negCenterid  = dfNegCentre.collect()[0][0]\n",
    "\n",
    "print(\"Center sentence from Negative class with minimum average distance\")\n",
    "dfNegCentre.show()\n",
    "\n",
    "dfNegClosest = dfNegDistances.where(col(\"id_origin\") == negCenterid) \\\n",
    "                             .orderBy(\"distances\") \\\n",
    "                             .limit(10)\n",
    "\n",
    "print(\"Ten closest sentences from negative class centre:\")\n",
    "\n",
    "dfNegClosest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
